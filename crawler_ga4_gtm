import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
import time
import csv
import random

# Função para verificar se a página contém o código de GA4
def check_ga4(content):
    return 'https://www.googletagmanager.com/gtag/js?id=G-' in content

# Função para verificar se a página contém o código do GTM
def check_gtm(content):
    return 'https://www.googletagmanager.com/gtm.js' in content

# Função para extrair links internos de uma página
def get_internal_links(url, domain):
    internal_links = set()  # Usar um set para evitar duplicações
    response = requests.get(url)  # Faz a requisição HTTP para a URL fornecida
    soup = BeautifulSoup(response.content, 'html.parser')  # Faz o parse do conteúdo HTML
    
    # Encontrar todos os links na página
    for link in soup.find_all('a', href=True):
        href = link['href']
        # Verificar se o link é interno (pertence ao mesmo domínio)
        full_url = urljoin(url, href)  # Converte links relativos em links absolutos
        parsed_url = urlparse(full_url)  # Faz o parse da URL para extrair o domínio
        if parsed_url.netloc == domain:  # Verifica se o domínio do link é o mesmo da URL inicial
            internal_links.add(full_url)  # Adiciona o link ao conjunto de links internos
    
    return internal_links

# Função principal para realizar o crawling do site
def crawl_website(start_url):
    visited = set()  # Conjunto de URLs já visitadas
    to_visit = {start_url}  # Conjunto de URLs a serem visitadas (inicia com a URL principal)
    domain = urlparse(start_url).netloc  # Extrai o domínio da URL inicial
    results = []  # Lista que armazenará os resultados (URL, GA4 e GTM)

    # Enquanto houver URLs a serem visitadas
    while to_visit:
        current_url = to_visit.pop()  # Retira uma URL da lista de URLs a visitar
        
        # Ignora a URL se já tiver sido visitada
        if current_url in visited:
            continue
        
        visited.add(current_url)  # Marca a URL como visitada

        try:
            # Faz uma requisição HTTP para a URL atual
            response = requests.get(current_url)
            content = response.text  # Obtém o conteúdo da página
            
            # Verifica se a página contém o código do GA4 e GTM
            has_ga4 = check_ga4(content)
            has_gtm = check_gtm(content)
            
            # Adiciona o resultado (URL, GA4 e GTM) à lista de resultados
            results.append({
                'url': current_url,
                'has_ga4': has_ga4,
                'has_gtm': has_gtm
            })
            
            # Extrai os links internos da página atual
            internal_links = get_internal_links(current_url, domain)
            to_visit.update(internal_links)  # Adiciona os novos links internos à lista de URLs a serem visitadas
        
        except requests.RequestException as e:
            print(f"Erro ao acessar {current_url}: {e}")  # Exibe erro caso a URL não possa ser acessada
        
        # Pausa de 1 segundo entre as requisições para não sobrecarregar o servidor
        time.sleep(random.uniform(1,6))
    
    return results  # Retorna a lista de resultados (URLs, GA4 e GTM)

# Iniciar o crawler a partir da URL principal
start_url = 'https://exemplo.com'  # Substitua pelo URL do seu site
resultados = crawl_website(start_url)  # Chama a função de crawling e armazena os resultados

# Exibir os resultados
# Exibe cada URL, GA4 e GTM
for resultado in resultados:
    print(f"URL: {resultado['url']} - GA4: {'Yes' if resultado['has_ga4'] else 'No'} - GTM: {'Yes' if resultado['has_gtm'] else 'No'}")

# Função para salvar os resultados em um arquivo CSV
# Essa função grava os resultados do crawler em um arquivo CSV com separador ";"
def save_to_csv(results, filename="resultados_ga4_gtm.csv"):
    headers = ['URL', 'GA4', 'GTM']  # Cabeçalhos das colunas no CSV
    
    # Abre o arquivo CSV em modo de escrita
    with open(filename, mode='w', newline='', encoding='utf-8') as file:
        writer = csv.writer(file, delimiter=';')  # Define o delimitador como ponto e vírgula
        writer.writerow(headers)  # Escreve o cabeçalho no arquivo
        
        # Escreve os dados (URL, GA4 e GTM) no arquivo CSV
        for result in results:
            # Substitui True/False por "Yes"/"No" para uma saída mais amigável
            ga4_status = 'Yes' if result['has_ga4'] else 'No'
            gtm_status = 'Yes' if result['has_gtm'] else 'No'
            writer.writerow([result['url'], ga4_status, gtm_status])  # Escreve cada linha de dados

# Chama a função para salvar os resultados no arquivo CSV
save_to_csv(resultados)  # Salva os resultados no arquivo "resultados_ga4_gtm.csv"
